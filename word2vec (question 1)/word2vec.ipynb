{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting exercise 1, set up python venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Using cached numpy-2.1.2-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-17.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.10.10-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\john1\\desktop\\sc4002-nlp-g14\\.venv\\lib\\site-packages (from datasets) (24.1)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.16.0-cp311-cp311-win_amd64.whl.metadata (65 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\john1\\desktop\\sc4002-nlp-g14\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\john1\\desktop\\sc4002-nlp-g14\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\john1\\desktop\\sc4002-nlp-g14\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\john1\\desktop\\sc4002-nlp-g14\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
      "  Using cached propcache-0.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Using cached datasets-3.0.2-py3-none-any.whl (472 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached aiohttp-3.10.10-cp311-cp311-win_amd64.whl (381 kB)\n",
      "Using cached huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached numpy-2.1.2-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Using cached pyarrow-17.0.0-cp311-cp311-win_amd64.whl (25.2 MB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-win_amd64.whl (101 kB)\n",
      "Using cached frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached yarl-1.16.0-cp311-cp311-win_amd64.whl (89 kB)\n",
      "Using cached propcache-0.2.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, tqdm, pyyaml, propcache, numpy, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, pyarrow, pandas, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.4.0 datasets-3.0.2 dill-0.3.8 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 huggingface-hub-0.26.1 idna-3.10 multidict-6.1.0 multiprocess-0.70.16 numpy-2.1.2 pandas-2.2.3 propcache-0.2.0 pyarrow-17.0.0 pytz-2024.2 pyyaml-6.0.2 requests-2.32.3 tqdm-4.66.5 tzdata-2024.2 urllib3-2.2.3 xxhash-3.5.0 yarl-1.16.0\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Using cached smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Using cached gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "Using cached smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.0.5 wrapt-1.16.0\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.9.11-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\john1\\desktop\\sc4002-nlp-g14\\.venv\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\john1\\desktop\\sc4002-nlp-g14\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.9.11-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11\n"
     ]
    }
   ],
   "source": [
    "#Installing packages\n",
    "!pip install datasets\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\john1\\Desktop\\SC4002-NLP-G14\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Load in dataset from huggingface\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "validation_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'effective but too-tepid biopic', 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .', \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\"], 'label': [1, 1, 1, 1, 1]}\n",
      "{'text': ['compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .', 'the soundtrack alone is worth the price of admission .', 'rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .', \"beneath the film's obvious determination to shock at any cost lies considerable skill and determination , backed by sheer nerve .\", 'bielinsky is a filmmaker of impressive talent .'], 'label': [1, 1, 1, 1, 1]}\n",
      "{'text': ['lovingly photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .', 'consistently clever and suspenseful .', 'it\\'s like a \" big chill \" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .', 'the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .', 'red dragon \" never cuts corners .'], 'label': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[:5])\n",
    "print(validation_dataset[:5])\n",
    "print(test_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold processed sentences\n",
    "processed_sentences = []\n",
    "\n",
    "# Iterate over the text data in the training dataset\n",
    "for text in train_dataset['text']:\n",
    "        # Preprocess the sentence (tokenization, lowercasing, removing punctuation)\n",
    "        processed_sentence = simple_preprocess(text)\n",
    "        \n",
    "        # Append the processed sentence to the list\n",
    "        processed_sentences.append(processed_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'rock', 'is', 'destined', 'to', 'be', 'the', 'st', 'century', 'new', 'conan', 'and', 'that', 'he', 'going', 'to', 'make', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', 'jean', 'claud', 'van', 'damme', 'or', 'steven', 'segal'], ['the', 'gorgeously', 'elaborate', 'continuation', 'of', 'the', 'lord', 'of', 'the', 'rings', 'trilogy', 'is', 'so', 'huge', 'that', 'column', 'of', 'words', 'cannot', 'adequately', 'describe', 'co', 'writer', 'director', 'peter', 'jackson', 'expanded', 'vision', 'of', 'tolkien', 'middle', 'earth'], ['effective', 'but', 'too', 'tepid', 'biopic'], ['if', 'you', 'sometimes', 'like', 'to', 'go', 'to', 'the', 'movies', 'to', 'have', 'fun', 'wasabi', 'is', 'good', 'place', 'to', 'start'], ['emerges', 'as', 'something', 'rare', 'an', 'issue', 'movie', 'that', 'so', 'honest', 'and', 'keenly', 'observed', 'that', 'it', 'doesn', 'feel', 'like', 'one']]\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 processed sentences\n",
    "print(processed_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.09414152 -0.11878416 -0.05333992  0.08578927  0.15098295 -0.12897322\n",
      " -0.06540246  0.3236996  -0.01549186  0.0365608  -0.09851414 -0.18797259\n",
      " -0.04587238  0.07860981 -0.05475076 -0.1708089  -0.07464073  0.04290609\n",
      "  0.0892233  -0.28068775  0.10067932 -0.09049039  0.08615655  0.03191651\n",
      "  0.09797062 -0.03810986  0.04082396  0.00925039 -0.21779495  0.04860101\n",
      "  0.20538682  0.02592665  0.15097454 -0.0644428   0.00112083  0.00932594\n",
      "  0.12944485 -0.02869492 -0.0669651  -0.17002709 -0.14821316  0.01565111\n",
      " -0.04059483  0.09088931  0.21613142 -0.10408204 -0.02004518 -0.09715451\n",
      "  0.07532046  0.1581658   0.09644075 -0.12946565 -0.07594289 -0.18816158\n",
      "  0.12671258 -0.06852089  0.03705452 -0.18167508 -0.22772323 -0.08048236\n",
      " -0.03098633 -0.06904349  0.0078952  -0.01599832 -0.28506088  0.14409557\n",
      " -0.02592292  0.24749775 -0.23820421  0.21418281  0.01842393  0.12119021\n",
      "  0.12428109  0.07352114  0.09557317  0.01320446  0.21330158 -0.12602869\n",
      " -0.33867034 -0.08845899 -0.07650428 -0.12566009 -0.05749847  0.28275612\n",
      " -0.1354619  -0.02653136  0.06247936  0.08552351 -0.01051921 -0.00231433\n",
      "  0.06657168  0.10081535  0.21708375  0.15770039  0.25521705  0.18153901\n",
      "  0.17625463 -0.08114754  0.11612523  0.08029829 -0.25520054  0.33076897\n",
      "  0.11107345 -0.05137559 -0.15525018 -0.29699793 -0.00445517  0.23534888\n",
      " -0.08179554 -0.36252445  0.04156425 -0.20548517 -0.11203581 -0.00316814\n",
      "  0.14325292 -0.03673556  0.15127355 -0.30142617 -0.04206478 -0.13047487\n",
      " -0.02442557  0.17647564  0.1532794  -0.0883993  -0.0773368   0.15501064\n",
      " -0.11461624 -0.08428209  0.04173063  0.0363615   0.0540939  -0.0222427\n",
      " -0.13462129 -0.16212267 -0.12303445  0.17466734 -0.06750552 -0.13088432\n",
      " -0.05651506 -0.17425832  0.15718694 -0.22017318 -0.13528943 -0.06215\n",
      " -0.0531462  -0.00825331 -0.1491133   0.08411065  0.03786029  0.00477808\n",
      "  0.03470199 -0.09479948 -0.0610456   0.16070126 -0.2146953   0.18219016\n",
      "  0.16630878  0.14872198 -0.03430232 -0.00576751  0.13507035  0.04729651\n",
      " -0.03208194 -0.03014831  0.01572134  0.07941096  0.1428396  -0.00879481\n",
      " -0.00255875  0.0849601  -0.09863368  0.07556643  0.05566152 -0.04885659\n",
      "  0.13280892  0.08106883 -0.18820493  0.1614068   0.04801514  0.12267207\n",
      "  0.04622989 -0.01685902  0.04242268 -0.07415383  0.09369759  0.01507952\n",
      " -0.10276305  0.09934359  0.17582212  0.09482901  0.05053056 -0.06900975\n",
      " -0.06968561 -0.11541513  0.06291702  0.08830886  0.03521502 -0.13433327\n",
      " -0.04862827 -0.06682611]\n",
      "[ 0.360772   -0.43440828 -0.2596736   0.37241513  0.53067213 -0.48808715\n",
      " -0.21397147  1.3031455  -0.11305947  0.11680369 -0.34948316 -0.6905844\n",
      " -0.20519742  0.29042774 -0.2162441  -0.5940322  -0.2953176   0.15196136\n",
      "  0.27516997 -1.0602608   0.3516201  -0.28925562  0.31529903  0.06664555\n",
      "  0.4544632  -0.11958075  0.13670251  0.10044293 -0.83255446  0.20883244\n",
      "  0.7354071   0.09303382  0.61589795 -0.24859002 -0.02482533 -0.01522646\n",
      "  0.53972745 -0.04795032 -0.25759438 -0.64384764 -0.5810323   0.06091416\n",
      " -0.24900964  0.2922384   0.7663221  -0.41363958 -0.05298496 -0.36292395\n",
      "  0.25480995  0.61682796  0.38545978 -0.43264008 -0.3296618  -0.8209368\n",
      "  0.5136113  -0.280161    0.17474388 -0.7232874  -0.85901654 -0.19896048\n",
      " -0.1676235  -0.22883447 -0.01178298 -0.05834698 -1.0920153   0.5147599\n",
      " -0.09601011  0.9515067  -0.87862855  0.7700868   0.06522736  0.45185962\n",
      "  0.47606274  0.19490089  0.360425    0.01839874  0.8359956  -0.44980332\n",
      " -1.2090353  -0.4320263  -0.2771892  -0.5017773  -0.1707278   1.0515381\n",
      " -0.58014554 -0.12508298  0.24338779  0.21753143 -0.08558314  0.00886969\n",
      "  0.2767787   0.40266958  0.8899372   0.61776865  1.0392463   0.6810218\n",
      "  0.7748826  -0.31252825  0.44531482  0.33562464 -1.0591608   1.384002\n",
      "  0.4199184  -0.16291742 -0.60510784 -1.1150254  -0.01278157  0.9915186\n",
      " -0.26377016 -1.4537473   0.17905982 -0.8160229  -0.42677528  0.08128935\n",
      "  0.57111514 -0.11685541  0.6441275  -1.2019848  -0.23358785 -0.44498894\n",
      " -0.12893277  0.61926967  0.6675573  -0.35471115 -0.33719838  0.61772305\n",
      " -0.5431548  -0.35806447  0.16999483  0.21739537  0.26507026 -0.17960934\n",
      " -0.55935717 -0.68481773 -0.47950602  0.7152528  -0.20393291 -0.4814537\n",
      " -0.16117461 -0.596626    0.6464248  -0.92617327 -0.53943926 -0.25751895\n",
      " -0.29762104 -0.03874026 -0.57622063  0.33004615  0.14084594  0.06784667\n",
      "  0.19082579 -0.4197341  -0.2708974   0.6269486  -0.7906751   0.7828204\n",
      "  0.7027489   0.5716288  -0.14331141 -0.05196392  0.5200654   0.16216242\n",
      " -0.13036567 -0.06881003  0.02467308  0.30152437  0.6304893  -0.04061302\n",
      "  0.01724817  0.39048567 -0.35659838  0.28533003  0.1939298  -0.19159758\n",
      "  0.5499059   0.29237342 -0.828506    0.6485784   0.21334718  0.42769322\n",
      "  0.19768468 -0.03483837  0.26747975 -0.3615258   0.3350519   0.03730527\n",
      " -0.44272912  0.37545747  0.6711682   0.39366475  0.1495876  -0.29967856\n",
      " -0.25687808 -0.48559722  0.20250575  0.4420555   0.09401879 -0.5220282\n",
      " -0.2547472  -0.19190218]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Initialize the model\n",
    "model = Word2Vec(\n",
    "    vector_size=200,  # Dimensionality of the word vectors\n",
    "    window=10,        # Maximum distance between current and predicted word\n",
    "    min_count=2,      # Ignores all words with a total frequency lower than this\n",
    "    workers=4         # Number of CPU cores to use\n",
    ")\n",
    "\n",
    "# Build vocabulary and train the model in one step\n",
    "model.build_vocab(processed_sentences)\n",
    "model.train(processed_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"word2vec_model.model\")\n",
    "\n",
    "# get vector for a specific word\n",
    "print(model.wv[\"rock\"])\n",
    "print(model.wv[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 0.9985318779945374), ('its', 0.9984530210494995), ('of', 0.9983338713645935), ('his', 0.9982286691665649), ('performances', 0.9978141188621521), ('characters', 0.9976279735565186), ('in', 0.9974678754806519), ('by', 0.997442901134491), ('with', 0.9973987936973572), ('most', 0.9973676204681396)]\n"
     ]
    }
   ],
   "source": [
    "# can also use model.wv.most_similar(word) to get most similar words\n",
    "\n",
    "print(model.wv.most_similar(\"the\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 8681\n"
     ]
    }
   ],
   "source": [
    "#Question 1\n",
    "\n",
    "#Size of vocab\n",
    "\n",
    "vocab_size = len(model.wv)\n",
    "print(f\"Size of the vocabulary: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words: 7607\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "\n",
    "#Count all OOV words\n",
    "all_words = set(word for sentence in processed_sentences for word in sentence)\n",
    "oov_words = [word for word in all_words if word not in model.wv]\n",
    "oov_count = len(oov_words)\n",
    "\n",
    "print(f\"Number of OOV words: {oov_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common strategy to mitigate the OOV problem without using transformer-based models is to utilize subword embeddings, such as those produced by the FastText model. \n",
    "\n",
    "FastText represents words as bags of character n-grams, which helps in understanding and generating embeddings for OOV words based on their subword components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Train a FastText model instead of Word2Vec\n",
    "fasttext_model = FastText(sentences=processed_sentences, vector_size=200, window=10, min_count=2, sg=1)\n",
    "\n",
    "# Save the FastText model\n",
    "fasttext_model.save(\"fasttext_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 8681\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(fasttext_model.wv)\n",
    "print(f\"Size of the vocabulary: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words: 0\n"
     ]
    }
   ],
   "source": [
    "all_words = set(word for sentence in processed_sentences for word in sentence)\n",
    "oov_words = [word for word in all_words if word not in fasttext_model.wv]\n",
    "oov_count = len(oov_words)\n",
    "\n",
    "print(f\"Number of OOV words: {oov_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
